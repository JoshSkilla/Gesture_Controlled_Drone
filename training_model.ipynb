{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4f0ca7-899d-4bd4-9565-695ab5359f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4125fbff-0264-49ee-97eb-7d82bf531c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b904b5a7-8f1f-4fce-8f48-a34ddabfd4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e74c55-71a3-465b-9a24-582adfb520d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='C:/Users/joshu/OneDrive/A levels/Other/Crest/Crest code/imgRecognition/signal_frames/silhouettes'\n",
    "test_path='C:/Users/joshu\\OneDrive/A levels/Other/Crest/Crest code/imgRecognition/signal_frames/test_silhouettes'\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb883234-6c08-4f5c-ae50-5b0c28da99b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['backwards', 'downwards', 'forwards', 'left', 'right', 'upwards']\n"
     ]
    }
   ],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb695e5-4f83-4d62-9400-4eb5b4a8e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Network\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=6):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        #((width-kernel_size+2Padding)/stride) +1\n",
    "        \n",
    "        #Input shape= (64,1,150,150)\n",
    "        #batch_size = 64\n",
    "        #channels =1 hopefully not 3 fix?\n",
    "        #height/width=150\n",
    "        #stride=1\n",
    "        #Padding=1\n",
    "        #kernel_size(f)=3\n",
    "        #(batch size,channels,height width)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (64,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #normalisation\n",
    "        #Shape= (64,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape= (64,12,150,150)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (64,12,75,75)\n",
    "        \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (64,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (64,20,75,75)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (64,32,75,75)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (64,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (64,32,75,75)\n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        #Feed forward function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "        #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa089b9-9755-446d-aed1-4e7e2f941447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb1698e-aa25-4d5d-8a19-7f63f1649efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8246366-a837-468f-a270-2de69dd8705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3651a7-d2f5-475c-9c6f-532bf954f960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200 300\n"
     ]
    }
   ],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))\n",
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c04c56-3489-44e9-b9ee-dd14c3e274cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(2.5859) Train Accuracy: 0.8861904761904762 Test Accuracy: 0.9866666666666667\n",
      "Epoch: 1 Train Loss: tensor(0.0231) Train Accuracy: 0.9964285714285714 Test Accuracy: 0.99\n",
      "Epoch: 2 Train Loss: tensor(0.0024) Train Accuracy: 0.9995238095238095 Test Accuracy: 0.9866666666666667\n",
      "Epoch: 3 Train Loss: tensor(0.0267) Train Accuracy: 0.995 Test Accuracy: 0.9933333333333333\n",
      "Epoch: 4 Train Loss: tensor(0.0240) Train Accuracy: 0.9973809523809524 Test Accuracy: 0.99\n",
      "Epoch: 5 Train Loss: tensor(0.0328) Train Accuracy: 0.9957142857142857 Test Accuracy: 0.9466666666666667\n",
      "Epoch: 6 Train Loss: tensor(0.0458) Train Accuracy: 0.9952380952380953 Test Accuracy: 0.99\n",
      "Epoch: 7 Train Loss: tensor(0.0082) Train Accuracy: 0.9985714285714286 Test Accuracy: 0.98\n",
      "Epoch: 8 Train Loss: tensor(0.0034) Train Accuracy: 0.9992857142857143 Test Accuracy: 0.99\n",
      "Epoch: 9 Train Loss: tensor(0.0008) Train Accuracy: 0.9997619047619047 Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "       \n",
    "        #if torch.cuda.is_available():\n",
    "        #    images=Variable(images.cuda())\n",
    "        #    labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        #if torch.cuda.is_available():\n",
    "        #    images=Variable(images.cuda())\n",
    "        #    labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "    \n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.pth')\n",
    "        best_accuracy=test_accuracy\n",
    "    \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
